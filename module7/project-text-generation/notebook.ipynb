{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m DROP_OUT \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m     14\u001b[0m DATASET_DIR \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39maclImdb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 15\u001b[0m TRAIN_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_DIR, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m TEST_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_DIR, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m CLASSES \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 80\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "N_HEADS = 8\n",
    "FF_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROP_OUT = 0.1\n",
    "\n",
    "DATASET_DIR = 'aclImdb'\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "CLASSES = ['neg', 'pos']\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = 'model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files: 50000\n"
     ]
    }
   ],
   "source": [
    "file_path = []\n",
    "for SUBSET_DIR in [TRAIN_DIR, TEST_DIR]:\n",
    "    for CLASS in CLASSES:\n",
    "        CLASS_DIR = os.path.join(SUBSET_DIR, CLASS)\n",
    "        for filename in os.listdir(CLASS_DIR):\n",
    "            file_path.append(os.path.join(CLASS_DIR, filename))\n",
    "\n",
    "print(f'Number of text files: {len(file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\n",
    "    Reference: https://stackoverflow.com/a/9662362\n",
    "\n",
    "    Args:\n",
    "        text (str): input string/document\n",
    "\n",
    "    Returns:\n",
    "        str: string without html tags\n",
    "        \n",
    "    \"\"\"\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def standardization(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove newline\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # remove html tags\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.index = 0\n",
    "        self.__init_special_tokens()\n",
    "        \n",
    "        print(self.vocab)\n",
    "        print(self.reverse_vocab)\n",
    "    \n",
    "    def __init_special_tokens(self):\n",
    "        special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = self.index\n",
    "            self.index += 1\n",
    "        \n",
    "        for k, v in self.vocab.items():\n",
    "            self.reverse_vocab[v] = k\n",
    "        \n",
    "    def fit_text(self, text):\n",
    "        for word in text.split(' '):\n",
    "            if word not in self.vocab:\n",
    "                if self.vocab_size <= self.index:\n",
    "                    break\n",
    "                \n",
    "                self.vocab[word] = self.index\n",
    "                self.reverse_vocab[self.index] = word\n",
    "                self.index += 1\n",
    "                \n",
    "    def fit_corpus(self, corpus):\n",
    "        for text in corpus:\n",
    "            self.fit_text(text)\n",
    "    \n",
    "    def encode(self, text, add_sos=False, get_mask=False):\n",
    "        seq = []\n",
    "        for word in text.split():\n",
    "            if word in self.vocab:\n",
    "                seq.append(self.vocab[word])\n",
    "            else:\n",
    "                seq.append(self.vocab['<UNK>'])\n",
    "                \n",
    "        if len(seq) > self.max_seq_len:\n",
    "            mask = [1] * self.max_seq_len\n",
    "            seq = seq[:self.max_seq_len]\n",
    "        else:\n",
    "            mask = [1] * len(seq) + [0] * (self.max_seq_len - len(seq))\n",
    "            seq = seq + [self.vocab['<PAD>']] * (self.max_seq_len - len(seq))\n",
    "            \n",
    "        if add_sos:\n",
    "            seq = [self.vocab['<SOS>']] + seq[:-1]\n",
    "            mask = [1] + mask[:-1]\n",
    "        \n",
    "        if get_mask:\n",
    "            return torch.tensor(seq), torch.tensor(mask)\n",
    "        else:\n",
    "            return torch.tensor(seq)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.vocab[key]\n",
    "    \n",
    "    def decode(self, seq):\n",
    "        if type(seq) == torch.Tensor:\n",
    "            seq = seq.numpy()\n",
    "        return ' '.join([self.reverse_vocab[i] for i in seq if i != self.vocab['<PAD>']])\n",
    "        \n",
    "    def from_json(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab = data[\"vocab\"]\n",
    "            self.reverse_vocab = data[\"reverse_vocab\"]\n",
    "            self.index = data[\"index\"]\n",
    "            self.vocab_size = data[\"vocab_size\"]\n",
    "            self.max_seq_len = data[\"max_seq_len\"]\n",
    "            \n",
    "    def to_json(self, path):\n",
    "        data = {\n",
    "            \"vocab\": self.vocab,\n",
    "            \"reverse_vocab\": self.reverse_vocab,\n",
    "            \"index\": self.index,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"max_seq_len\": self.max_seq_len,\n",
    "        }\n",
    "        with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
      "{0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(VOCAB_SIZE, MAX_SEQ_LEN)\n",
    "\n",
    "tokenizer.fit_corpus([standardization(open(path, 'r', encoding='utf8').read()) for path in file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: tensor([   2,  211,   20,    6, 2731, 4892,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Mask: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Encoded text: <SOS> this is a test sentence\n"
     ]
    }
   ],
   "source": [
    "text = 'this is a test sentence'\n",
    "\n",
    "encoded_text, mask = tokenizer.encode(text, add_sos=True, get_mask=True)\n",
    "print(f'Encoded text: {encoded_text}')\n",
    "print(f'Mask: {mask}')\n",
    "print(f'Encoded text: {tokenizer.decode(encoded_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_input_labels(text):\n",
    "    input_ids, input_mask = tokenizer.encode(text, add_sos=True, get_mask=True)\n",
    "    labels, label_mask = tokenizer.encode(text, add_sos=False, get_mask=True)\n",
    "    return input_ids, input_mask, labels, label_mask\n",
    "\n",
    "class TextGenerationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_paths, seq_len, tokenizer, standardize=True, get_mask=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.standardize = standardize\n",
    "        self.get_mask = get_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.file_paths[idx], 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        if self.standardize:\n",
    "            text = standardization(text)\n",
    "        \n",
    "        input_ids, input_mask, labels, label_mask = prepare_lm_input_labels(text)\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'target_ids': labels,\n",
    "        }\n",
    "        if self.get_mask:\n",
    "            sample['input_mask'] = input_mask\n",
    "            sample['target_mask'] = label_mask\n",
    "            \n",
    "        return sample\n",
    "        \n",
    "        # return self.tokenizer.encode(text, add_sos=True, get_mask=self.get_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextGenerationDataset(file_path, MAX_SEQ_LEN, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: tensor([    2,   518,   333,   144, 13278,     1,   788,     1,    88,  5208,\n",
      "         5208,     1,   495,   315,   280,   157,   323,  8459,     5,   144,\n",
      "            1,     1,  3847,   211,   473,    30,   527,  1897,   336,   120,\n",
      "         2929,   120,  7245,  8887,   226,   218,  1055,     6,  4539,   226,\n",
      "          218, 11089,   241,   226,   218,  4893,   120,  1417,   546, 13278,\n",
      "         1279,  2432,  1204, 12024,   317,   426,    98,   336,    41,  1164,\n",
      "           35,  1660,  1267, 13412,  4059,    16, 12944,  1772,  2771,   877,\n",
      "          877,   226,  2312,  1654,   495,   218,   708,    67,  4557,  1152])\n",
      "Encoded target: tensor([  518,   333,   144, 13278,     1,   788,     1,    88,  5208,  5208,\n",
      "            1,   495,   315,   280,   157,   323,  8459,     5,   144,     1,\n",
      "            1,  3847,   211,   473,    30,   527,  1897,   336,   120,  2929,\n",
      "          120,  7245,  8887,   226,   218,  1055,     6,  4539,   226,   218,\n",
      "        11089,   241,   226,   218,  4893,   120,  1417,   546, 13278,  1279,\n",
      "         2432,  1204, 12024,   317,   426,    98,   336,    41,  1164,    35,\n",
      "         1660,  1267, 13412,  4059,    16, 12944,  1772,  2771,   877,   877,\n",
      "          226,  2312,  1654,   495,   218,   708,    67,  4557,  1152,    41])\n",
      "Decoded input: <SOS> movie about two australian <UNK> nell <UNK> and sue sue <UNK> what happens when they become girlfriends of two <UNK> <UNK> caught this at an art cinema here in america in 1981 technically i was still a teenager i was 19 so i was interested in seeing how australian teens acted script wise theres nothing new here it shows the usual teenage adventures dealing with dating sex suicide etc etc i always knew what was going to happen before\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(dataset))\n",
    "\n",
    "sample = dataset[idx]\n",
    "encoded_input = sample['input_ids']\n",
    "encoded_target = sample['target_ids']\n",
    "print(f'Encoded input: {encoded_input}')\n",
    "print(f'Encoded target: {encoded_target}')\n",
    "print(f'Decoded input: {tokenizer.decode(encoded_input)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "\n",
    "    Args:\n",
    "        sz (int): the size of the mask\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: the mask tensor\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "generate_square_subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.multi_head_attn = nn.MultiheadAttention(\n",
    "            embedding_dim, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # make a mask for padding tokens\n",
    "        key_padding_mask = (1 - mask).bool() if mask is not None else None\n",
    "        context_vector, _ = self.multi_head_attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            attn_mask=generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        )\n",
    "        context_vector = self.dropout1(context_vector)\n",
    "        out1 = self.layer_norm1(x + context_vector)\n",
    "        \n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        \n",
    "        return self.layer_norm2(out1 + ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, embedding_dim):\n",
    "        super(TokenAndPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(x.device)\n",
    "        return self.token_embedding(x) + self.pos_embedding(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, max_seq_len, embedding_dim, n_heads, ff_dim, n_layers, dropout=0.1\n",
    "    ):\n",
    "        super(GeneratorModel, self).__init__()\n",
    "        \n",
    "        self.embedding = TokenAndPositionalEmbedding(vocab_size, max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embedding_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        torch.nn.init.normal_(m.weight, std=0.02)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        torch.nn.init.normal_(m.weight, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    avg_loss = 0\n",
    "    pbar = tqdm(data_loader)\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        input_mask = batch['input_mask'].to(device)\n",
    "        target_mask = batch['target_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, input_mask)\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        target_ids = target_ids.reshape(-1)\n",
    "        target_mask = target_mask.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        avg_loss = (0.9 * avg_loss+ 0.1 * loss.item()) / (1 - 0.9 ** (len(losses)))\n",
    "        pbar.set_description(f'Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}')\n",
    "        \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d03fec353ae4733b2e288c58ebcdbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 8.7503\n",
      "####################################################################################################\n",
      "Training Epoch: 02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a923ce3d0b245eda29a31a939c0a2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 6.4457\n",
      "####################################################################################################\n",
      "Training Epoch: 03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b6e98c5e1a4fba8af3510aed650699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train Loss: 5.9942\n",
      "####################################################################################################\n",
      "Training Epoch: 04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b484023f9d942cba89947feaa51a815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, dataloader, optimizer, critetion, DEVICE)\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     23\u001b[0m avg_loss \u001b[39m=\u001b[39m ((\u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m avg_loss) \u001b[39m+\u001b[39m (\u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m loss\u001b[39m.\u001b[39mitem())) \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(losses)))\n\u001b[0;32m     24\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Avg Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = TextGenerationDataset(file_path, MAX_SEQ_LEN, tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = GeneratorModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROP_OUT\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "critetion = nn.CrossEntropyLoss(ignore_index=tokenizer['<PAD>'])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Training Epoch: {epoch+1:02}')\n",
    "    train_loss = train(model, dataloader, optimizer, critetion, DEVICE)\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.4f}')\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"model.pt\"))\n",
    "tokenizer.to_json_file(os.path.join(OUTPUT_DIR, \"tokenizer.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(logits, top_k=10):\n",
    "    logits, indices = torch.topk(logits, top_k)\n",
    "    indices = np.asarray(indices).astype(np.int32)\n",
    "    preds = torch.softmax(logits, dim=-1).numpy()\n",
    "    preds = np.asarray(preds).astype(np.float32)\n",
    "    return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generate(start_tokens, max_generated_tokens):\n",
    "    start_tokens = [_ for _ in start_tokens]\n",
    "    num_tokens_generated_local = 0\n",
    "    tokens_generated = []\n",
    "    max_generated_tokens \n",
    "    while num_tokens_generated_local <= max_generated_tokens:\n",
    "        pad_len = MAX_SEQ_LEN - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = torch.tensor(x).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y = model(x)\n",
    "            y = y.cpu()\n",
    "            \n",
    "        sample_token = sample_from(y[0][sample_index])\n",
    "        \n",
    "        if sample_token == tokenizer['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated_local = len(tokens_generated)\n",
    "        \n",
    "    txt = tokenizer.decode(tokens_generated)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prompt = \"this movie is\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:len(start_prompt.split())]\n",
    "num_tokens_generated = 30\n",
    "self_max_tokens = 30\n",
    "\n",
    "generated_text = generate(start_tokens, max_generated_tokens=self_max_tokens)\n",
    "\n",
    "print(f'Prompt: {start_prompt}')\n",
    "print(f'Generated Text: {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4b1d2403d5bedfc2b499b2d1212ae0437b5f8ebf43026ed45c1b9608ddeb20c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
